{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import lightgbm as lgb\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\n",
    "class GroupTimeSeriesSplit(_BaseKFold):\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_size=None\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_size = max_train_size\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "        group_test_size = n_groups // n_folds\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "            for train_group_idx in unique_groups[:group_test_start]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "            train_end = train_array.size\n",
    "            if self.max_train_size and self.max_train_size < train_end:\n",
    "                train_array = train_array[train_end -\n",
    "                                          self.max_train_size:train_end]\n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T14:07:51.929270Z",
     "start_time": "2023-12-21T14:07:51.916548Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sales_train = pd.read_pickle(\"../data/fulling_connected_feature_eng_train_data.pkl\")\n",
    "sales_test = pd.read_pickle(\"../data/fulling_connected_feature_eng_test_data.pkl\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "COLUMNS_TO_DROP = [\"wm_yr_wk\", \"date\", \"weekday\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "outcome_col = \"sales_amount\"\n",
    "predictor_cols = [col for col in sales_train.columns if col !=  outcome_col]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_predictors = sales_train[predictor_cols]\n",
    "train_outcome = sales_train[outcome_col]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_predictors = sales_test[predictor_cols]\n",
    "test_outcome = sales_test[outcome_col]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ORDINAL_COLUMNS = [\n",
    "    \"item_id\",\n",
    "    \"is_weekday\",\n",
    "    \"is_weekend\",\n",
    "    \"is_holiday\",\n",
    "    \"price_category\",\n",
    "    \"7dl_price_category\",\n",
    "    \"event_name_1\",\n",
    "    \"event_type_1\",\n",
    "    \"event_name_2\",\n",
    "    \"event_type_2\",\n",
    "    \"7dl_event_name_1\",\n",
    "    \"7dl_event_name_2\",\n",
    "    \"7dl_event_type_1\",\n",
    "    \"7dl_event_type_2\",\n",
    "    \"snap_TX\"\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = lgb.LGBMRegressor()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "columns_transforms = make_column_transformer(\n",
    "        (OneHotEncoder(), ORDINAL_COLUMNS),\n",
    "        (\"drop\", COLUMNS_TO_DROP),\n",
    "        remainder='passthrough'\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_pipeline = Pipeline(\n",
    "        [\n",
    "            (\"column_transformation\", columns_transforms),\n",
    "            (\"model\", model),\n",
    "        ]\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tuning_parameters = {\n",
    "    \"model__max_depth\": [20, 50, 100, 200],\n",
    "    \"model__num_leaves\": [20, 40, 100, 120],\n",
    "    \"model__learning_rate\": [0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "    \"model__n_estimators\": [100, 500, 700, 1000],\n",
    "    \"model__colsample_bytree\": [0.3, 0.5, 0.7, 1]\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cv_split = TimeSeriesSplit(n_splits=5)\n",
    "grid_search = RandomizedSearchCV(\n",
    "    model_pipeline, tuning_parameters,\n",
    "    cv=cv_split,\n",
    "    scoring=[\"neg_mean_squared_error\", \"neg_mean_absolute_error\"],\n",
    "    refit=\"neg_mean_absolute_error\",\n",
    "    n_jobs=5\n",
    ")\n",
    "grid_search.fit(train_predictors, train_outcome)\n",
    "lgbm_model = grid_search.best_estimator_\n",
    "joblib.dump(lgbm_model, 'lgbm_model.joblib')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_pipeline = Pipeline(\n",
    "        [\n",
    "            (\"column_transformation\", columns_transforms),\n",
    "            (\"model\", GradientBoostingRegressor()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "tuning_parameters = {\n",
    "    \"model__max_depth\": [20, 50, 100, 200],\n",
    "    \"model__learning_rate\": [0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "    \"model__n_estimators\": [100, 500, 700, 1000],\n",
    "}\n",
    "\n",
    "cv_split = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "grid_search_xgboost = RandomizedSearchCV(\n",
    "    model_pipeline, tuning_parameters,\n",
    "    cv=cv_split,\n",
    "    scoring=[\"neg_mean_squared_error\", \"neg_mean_absolute_error\"],\n",
    "    refit=\"neg_mean_absolute_error\",\n",
    "    n_jobs=5\n",
    ")\n",
    "grid_search_xgboost.fit(train_predictors, train_outcome)\n",
    "xgboost_model = grid_search_xgboost.best_estimator_\n",
    "joblib.dump(xgboost_model, 'xgboost_model.joblib')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "lgbm_model = joblib.load('lgbm_model.joblib')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T15:00:12.987230Z",
     "start_time": "2023-12-21T15:00:12.961351Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE error was: 1.4645787446348453\n",
      "MSE error was: 11.542862484675576\n",
      "RMSE error was: 3.3974788424176503\n"
     ]
    }
   ],
   "source": [
    "train_predictions = lgbm_model.predict(train_predictors)\n",
    "mae_error = mean_absolute_error(train_outcome, train_predictions)\n",
    "mse_error = mean_squared_error(train_outcome, train_predictions)\n",
    "rmse_error = mean_squared_error(train_outcome, train_predictions, squared=False)\n",
    "print(f\"MAE error was: {mae_error}\")\n",
    "print(f\"MSE error was: {mse_error}\")\n",
    "print(f\"RMSE error was: {rmse_error}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T15:00:34.272414Z",
     "start_time": "2023-12-21T15:00:13.763632Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE error was: 1.7420153260332747\n",
      "MSE error was: 11.03325752696739\n",
      "RMSE error was: 3.321634767244495\n"
     ]
    }
   ],
   "source": [
    "test_predictions = lgbm_model.predict(test_predictors)\n",
    "mae_error = mean_absolute_error(test_outcome, test_predictions)\n",
    "mse_error = mean_squared_error(test_outcome, test_predictions)\n",
    "rmse_error = mean_squared_error(test_outcome, test_predictions, squared=False)\n",
    "print(f\"MAE error was: {mae_error}\")\n",
    "print(f\"MSE error was: {mse_error}\")\n",
    "print(f\"RMSE error was: {rmse_error}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T15:00:34.591328Z",
     "start_time": "2023-12-21T15:00:34.277112Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(test_size=train_predictors.shape[0] // (5 + 1))\n",
    "for i, (train_index, test_index) in enumerate(tscv.split(train_predictors.set_index(\"date\"), groups=train_predictors.item_id)):\n",
    "    train_split_1 = train_predictors.iloc[train_index, :]\n",
    "    vals_split_1 = train_predictors.iloc[test_index, :]\n",
    "    break\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T14:18:17.761735Z",
     "start_time": "2023-12-21T14:18:17.441670Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "data": {
      "text/plain": "(262399, 31)"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals_split_1.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T14:18:38.843960Z",
     "start_time": "2023-12-21T14:18:38.835Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "data": {
      "text/plain": "(262404, 31)"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_split_1.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T14:18:45.578194Z",
     "start_time": "2023-12-21T14:18:45.560012Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
