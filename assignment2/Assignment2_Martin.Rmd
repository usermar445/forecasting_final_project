---
header-includes:
- \usepackage{xcolor}
- \usepackage{color} 
- \usepackage{fancyhdr,color}
- \usepackage{lipsum}
- \fancyfoot[CE] {\thepage}
title: "**Assignment 1,2\n &nbsp;  \n Team X**"
subtitle: "Applied Forecasting in Complex Systems 2022"
author: Martin Arnold
date: "University of Amsterdam \n &nbsp;  \n November, 7, 2022 "
output: pdf_document
fontsize: 11pt
highlight: tango
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  cache = TRUE,
  dev.args = list(pointsize = 11)
)
options(digits = 3, width = 60)
library(fpp3)
library(latex2exp)
require(dplyr)
```
## Exercise 1
**1.1)**

```{r}
times <-  olympic_running %>% 
  as_tibble %>% 
  #select(-Sex) %>% 
  mutate(discipline = ifelse(Length<800, "Sprint", ifelse(Length<3000, "Middle-Distance", "Long-Distance"))) %>% 
  #select(-Length) %>% 
  #group_by(Year, Sex, discipline) %>%
  #summarize(time=mean(Time, na.rm=TRUE)) %>% 
  #ungroup() %>% 
  as_tsibble(index=Year, key=c(Sex, Length))

times %>% autoplot(Time) + facet_wrap(~discipline, nrow=3)
```
Using the following criteria: 
- Length < 800m Sprint
- 800 <= Length < 3000 Middle-Distance
- 3000 <= Length Long-Distance

We disregard missing values for women. 

The plot shows some gaps due to the absence of the world wars (1916, 1940-44). 

The long-distance discipline was added later. 

Sprint has the most constant winning times with a very slight upwards trends. 

Long-Distance has a spike in the late 80s and 90s but then settling back to normal levels. 

**1.2** 
```{r}
fit_times <- times %>%
   model(TSLM(Time ~ trend()))

fit_times %>% coef() %>% filter(term=="trend()")

```

Average decrease per year in seconds is equal to $\beta_1$, e.g. the estimate form the table above.

**1.3)**
```{r}
resid <- fit_times %>% residuals() %>% mutate(discipline = ifelse(Length<800, "Sprint", ifelse(Length<3000, "Middle-Distance", "Long-Distance"))) 
resid %>% autoplot(.resid) + facet_wrap(~discipline, nrow=3)
```
Some residuals show great variation for Long-distance, predicition bad. Good for Sprint

**1.4)**
```{r}
forecast <- forecast(fit_times)
forecast %>% filter(Year==2020)
```
Normally distributed, etc.

## Exercise 2

**2.1)**
We filter the global_economy by the country code "AFG", select the `Population`
column (and `Year`) and then create a `tsibble` object out of it, to then
`autoplot()` it:
```{r, fig.height=3.5}
pop <- global_economy %>% 
  filter(Code=="AFG") %>% 
  select(Year, Population) %>% 
  as_tsibble(index=Year)
pop_plot <- pop%>% autoplot(Population)
pop_plot + 
  labs(title="Population of Afghanistan", 
       subtitle="Yearly, 1069-2017", 
       y="Population")
```

From the time plot we can clearly see a strong (linear almost exponential) 
upwards trend with a very obvious dip in the 1980s which marks the Soviet-Afghan war. 
There is no seasonality.
((is the trend linear? or exponential?))

**2.2)**
We can create both models in one command using the `TSLM()` method, regressing
the `Population` on the `trend` (with one time specificng the knots)
```{r}
model_pop <- pop %>% 
  model(linear = TSLM(Population ~ trend()), 
        piecewise = TSLM(Population ~ trend(knots=c(1980, 1989))))
```

To compare the models we look at the regression results:
```{r}
model_pop %>% report()
```
For comparison, we look at the $adj. R^2$, we can see that the
piecewise model has a higher $adj. R^2$ and is in that regards the better model 
and captures the relationship between trend and population better. This is because
the linear model is not able to capture the dip from the war. 

((the exercise does not explicitly  state that we need to comment on anything here or come to a conclusion?
Anything other to compare? like residuals (but its not stated)?))


**2.3)**
To forecast, we use the `forecast()` method and plot the forecast:
```{r, fig.height=3.5}
fc_pop <- model_pop %>% forecast(h="5 years")
pop %>%
  autoplot(Population) +
  autolayer(fc_pop) +
  labs(
       title = "5-year forecast of Afghan population",
       subtitle = "Forecast 2018 - 2023, linear vs. piecewise",
       y = "Population",)
```

From the plot we can see that the piecwise model performs considerably better. 
The model captures the trend better and has much less uncertainty. The prediction interval
of the linear model is very large, meaning that there is alot of uncertainty assigned. 
Because we do not have the actual data for the forecast years, we cannot compute
the accuracy of the two models. 

((anything else to consider? residulas?))


## Exercise 3

**3.1)**
```{r}
arrivals <- aus_arrivals  %>% 
  as_tsibble(index=Quarter, key=Origin) 

arrivals_agg <- arrivals %>% index_by(Quarter) %>% summarize(arrivals = sum(Arrivals))
arrivals_agg %>% autoplot()
```
- increasing variance, therefore, box-cox transformation
- seasonality
- upwards trend from 1982 to mid 2000, then downwards trend


- Data is strictly positive, so we can use all (also error multiplicative errors) 
- chose the best model by minimizing AIC (using `ETS(arrivals)`). Resulting model is a `ETS(M, A, M)`
- or if by hand: compare ETS(A, A, M) to ETS(M, A, M). Trend should b A, because only short-term forecast (no dampening needed). Multiplicative because variance not constant of seasonality. 

```{r}
arrivals_train <- arrivals_agg %>% slice(1:(n()-2*4+1))
fit_arrivals <- arrivals_train %>%
  model(
    multiplicative1 = ETS(arrivals ~ error("M") + trend("A") + season("M")),
    multiplicative2 = ETS(arrivals ~ error("M") + trend("A") + season("A")))
report(fit_arrivals)

fc <- fit_arrivals %>% forecast(h= "2 years")

fit2 <- arrivals_train %>%
  model(ETS(arrivals))
report(fit2)

fc2 <- fit2 %>% forecast(h = "2 years")

fc2 %>%
  autoplot(arrivals_agg, level = NULL) +
  labs(title="Australian arrivals",
       y="Overnight trips (millions)") +
  guides(colour = guide_legend(title = "Forecast"))

```
- multiplicative seasonality needed because seasonal variance not constant across levels


**3.2)**  
```{r}
fit_all <- arrivals_train %>%
  model(
    ets = ETS(arrivals),
    ets_add = ETS(log(arrivals) ~ error("A") + trend("A") + season("A")),  
    naive = SNAIVE(arrivals ~lag("year")),
    ets_decomp = decomposition_model(
      STL(log(arrivals)),
      ETS(season_adjust))
    )

fc_all <- fit_all %>% forecast(h="2 years")

accuracy(fc_all, arrivals_agg)
```
- seasonal naive performs best ???


```{r}
fc_all %>%
  autoplot(arrivals_agg,
    level = NULL) +
  labs(y = "$US",
       title = "Arrivals something") +
  guides(colour = guide_legend(title = "Forecast"))
````
```{r}
fit_all %>% select(ets) %>%  gg_tsresiduals()
```
```{r}
fit_all %>% select(ets_add) %>%  gg_tsresiduals()
```
```{r}
fit_all %>% select(ets_decomp) %>%  gg_tsresiduals()
```

```{r}
fit_all %>% select(naive) %>%  gg_tsresiduals()
```


**3.3)**
```{r}
cv <- arrivals_agg %>%
  stretch_tsibble(.init = 20, .step = 1) |>
  relocate(Quarter, .id)

cv %>%
  model(
    ets = ETS(arrivals),
    ets_add = ETS(log(arrivals) ~ error("A") + trend("A") + season("A")),  
    naive = SNAIVE(arrivals ~lag("year")),
    ets_decomp = decomposition_model(
      STL(log(arrivals)),
      ETS(season_adjust))
    )  %>% forecast(h="2 years") %>% accuracy(arrivals_agg)

```


- in general, errors lower


## Exercise 4
**4.1)**
```{r}
gdp <- global_economy %>% filter(Code == "USA") %>% select(Year, GDP) %>% as_tsibble(index=Year)
gdp %>% autoplot()
```

```{r}
lambda <- gdp %>% 
  features(GDP, features=guerrero) %>% 
  pull(lambda_guerrero)

plot_gdp_transformed <- gdp %>% autoplot(box_cox(GDP, lambda)) + 
  labs(title="Quarterly gas production", 
       subtitle="In Australia, Q1 1956 - Q2 2010", 
       y="Transformed gas production")
gdp_transformed <- gdp %>% mutate(trans = box_cox(GDP, lambda))

plot_gdp_transformed
```
- makes it linear
- weird bump in 00 years


Simply apply ARIMA alogrithm
```{r}
gdp_fit <- gdp_transformed %>% model(stepwise = ARIMA(trans),
              search = ARIMA(trans, stepwise=FALSE))

glance(gdp_fit) %>% arrange(AICc)
```
- both equal

```{r}
gdp_fit
```
- ARIMA(0,2,2)

**b)**
-following 9.7

```{r}
gdp_diff <- gdp_transformed %>% mutate(diff = difference(trans))
gdp_diff %>% autoplot(diff)
```


Look 
```{r}
gdp_diff %>% gg_tsdisplay(diff, plot_type="partial")
```

- PACF not really sinosidial decaying
- therefore ARIMA(0, d, q)
- with d=1, q=1,2


```{r}
gdp_fit2 <- gdp_transformed %>% model(
  arima011 = ARIMA(trans ~ pdq(0,1,1)),
  arima012 = ARIMA(trans ~ pdq(0,1,2)),
  )

glance(gdp_fit2) %>% arrange(AICc)

```

- both have lower AICc, with AIRMA(0,1,2) the bet

```{r}
gdp_fit2 %>% select(arima012) %>% gg_tsresiduals()
```

- ok? -> white noise



```{r}
augment(gdp_fit2) %>% filter(.model=="arima012") %>%  features(.innov, ljung_box, lag = 10)

```

- large enough p value


**4.3)**

```{r}
fc_gdp <- gdp_fit2 %>%
  forecast(h="5 years") %>%
  filter(.model=='arima012') 

fc_gdp %>% autoplot()

gdp_transformed %>% autoplot(trans) + autolayer(fc_gdp)

```

```{r}
fc_ets_gdp <- gdp %>%
  model(
    ets = ETS(GDP))  %>% forecast(h="5 years")

gdp %>% autoplot(GDP) + autolayer(fc_ets_gdp)

```


- ARIMA less uncertainty

